Sistema multiagentes com CrewAI e Ollama (Phi-3 Mini) para criar e revisar arquiteturas de integra√ß√£o B2B/B2C com EntraID. Otimizado para a NVIDIA GeForce GTX 1050 Ti (4 GB VRAM), roda offline com acelera√ß√£o por GPU. Tr√™s agentes colaboram:
Arquiteto: Gera desenho de solu√ß√£o e ADR.
Tech Lead: Revisa a arquitetura para escalabilidade.
Gerente: Produz relat√≥rio de custo e benef√≠cios.
Logs detalhados rastreiam o racioc√≠nio dos agentes.
Funcionalidades
Multiagentes com CrewAI.
Infer√™ncia offline com Phi-3 Mini (~2-3 GB VRAM).
Acelera√ß√£o por GPU (CUDA).
Logs em reasoning_logs.log e terminal (verbose=2).
Foco em integra√ß√£o B2B/B2C com EntraID.
Requisitos
Hardware: GTX 1050 Ti (4 GB VRAM), 8 GB RAM.
OS: Windows 10/11.
Software: Python 3.8+, Ollama, NVIDIA Drivers, CUDA 11.8.
Depend√™ncias:
bash
pip install crewai litellm openai requests
Instala√ß√£o
Ollama:
bash
ollama pull phi3-mini
GPU:
Instale drivers NVIDIA (nvidia.com).
Instale CUDA 11.8 (developer.nvidia.com).
Opcional: $env:CUDA_VISIBLE_DEVICES = "0"
Reposit√≥rio:
bash
git clone https://github.com/seu-usuario/multiagentes.git
cd multiagentes
Ambiente Virtual:
bash
python -m venv .venv
.\venv\Scripts\activate
pip install crewai litellm openai requests
Uso
Inicie o Ollama:
bash
ollama serve
Execute:
bash
python agentes_arquitetura_offline.py
Monitore GPU:
bash
nvidia-smi
VRAM: ~2-3 GB.
GPU-Util: 50-90%.
Logs: Veja reasoning_logs.log e terminal.
Detalhes T√©cnicos
CrewAI: Orquestra agentes com tarefas sequenciais.
Ollama: Servidor local (http://localhost:11434) para Phi-3 Mini (4-bit, ~3.8B par√¢metros).
litellm: Gerencia chamadas ao Ollama.
GPU: GTX 1050 Ti, CUDA 11.8, CUDA_VISIBLE_DEVICES=0.
Logs: logging.DEBUG e verbose=2.
Exemplo de Sa√≠da
Arquiteto: "Desenho: Tenant B2B -> EntraID -> Tenant B2C. ADR: Contexto: Integra√ß√£o segura..."
Tech Lead: "Revis√£o: Adiciona API Gateway, corrige OAuth 2.0."
Gerente: "Relat√≥rio: Custo: $5.000/m√™s, 4 meses, seguran√ßa."
Extensibilidade
Adicione agentes (ex.: Seguran√ßa).
Teste Mistral-7B (4-bit, ~3-4 GB VRAM):
bash
ollama pull mistral --quantize q4_0
Novos casos: Planejamento, an√°lise de dados.
Limita√ß√µes
Phi-3 Mini gera sa√≠das simplificadas.
GTX 1050 Ti limita modelos maiores.
Configura√ß√£o inicial complexa (drivers, CUDA).
Contribui√ß√µes
Fork o reposit√≥rio.
Crie branch: git checkout -b feature/nova-funcionalidade.
Commit: git commit -m "Adiciona funcionalidade".
Pull request.
Licen√ßa
MIT License (LICENSE)
Contato
GitHub: seu-usuario
X: [
@seu
-usuario]
Publica√ß√£o no GitHub
Crie Reposit√≥rio:
Em github.com, crie multiagentes (P√∫blico/Privado).
Organize Arquivos:
Pasta multiagentes:
agentes_arquitetura_offline.py
README.md (este arquivo)
.gitignore:
.venv/
__pycache__/
*.log
*.pyc
Opcional: LICENSE (MIT)
Git:
bash
cd multiagentes
git init
git add .
git commit -m "Initial commit"
git remote add origin https://github.com/seu-usuario/multiagentes.git
git push -u origin main
Valide:
Acesse https://github.com/seu-usuario/multiagentes.
Confirme renderiza√ß√£o do README.md.
Divulgue:
Tags: AI, multiagent, CrewAI, Ollama, Phi-3.
Compartilhe no X: #AI #Multiagent #Ollama.
Pr√≥ximos Passos:
Publique com os passos acima.
Adicione screenshots (use ShareX).
Para extens√µes (ex.: novo agente), avise para ajustes.
Compartilhe o link do reposit√≥rio para feedback.
Se precisar de mais concis√£o, ajustes ou se√ß√µes espec√≠ficas, √© s√≥ pedir! üöÄ
