Multiagentes: Sistema de Arquitetura de Solu√ß√µes com CrewAI e Ollama

Vis√£o Geral
O reposit√≥rio multiagentes implementa um sistema multiagentes para automa√ß√£o de tarefas t√©cnicas, utilizando o framework CrewAI e o modelo de linguagem Phi-3 Mini executado localmente com o Ollama. O sistema √© projetado para criar, revisar e relatar arquiteturas de solu√ß√µes em nuvem, com foco na integra√ß√£o de tenants B2B e B2C usando o EntraID como provedor de identidade (IdP). Ele √© otimizado para rodar em hardware modesto, como a NVIDIA GeForce GTX 1050 Ti (4 GB de VRAM), com acelera√ß√£o por GPU via CUDA.
Tr√™s agentes colaboram sequencialmente:
Arquiteto de Solu√ß√µes: Gera um desenho de solu√ß√£o e um ADR (Architecture Decision Record) para a integra√ß√£o B2B/B2C.
Tech Lead: Revisa a arquitetura, corrigindo erros e sugerindo melhorias t√©cnicas para escalabilidade e viabilidade.
Gerente de Tecnologia: Produz um relat√≥rio com estimativas de custo, cronograma e benef√≠cios.
O projeto destaca-se por sua execu√ß√£o offline, eliminando a depend√™ncia de APIs externas (ex.: OpenAI), e por logs detalhados que permitem acompanhar o racioc√≠nio dos agentes. Ele √© ideal para desenvolvedores, engenheiros de TI e entusiastas de IA que desejam explorar sistemas multiagentes em ambientes com recursos limitados.
Funcionalidades
Sistema Multiagentes: Coordena√ß√£o de tr√™s agentes (Arquiteto, Tech Lead, Gerente) usando CrewAI para tarefas t√©cnicas colaborativas.
Execu√ß√£o Offline: Uso do Ollama com Phi-3 Mini (~3.8B par√¢metros, quantizado em 4-bit) para infer√™ncia local, garantindo privacidade e zero custo de API.
Acelera√ß√£o por GPU: Otimizado para a NVIDIA GeForce GTX 1050 Ti, com configura√ß√£o para for√ßar o uso da GPU via CUDA.
Logs Detalhados: Logging personalizado e modo verbose=2 do CrewAI para rastrear prompts, respostas e o racioc√≠nio dos agentes.
Integra√ß√£o B2B/B2C: Foco em arquiteturas de nuvem, com suporte para EntraID como IdP principal.
Modularidade: Estrutura extens√≠vel para adicionar novos agentes ou tarefas (ex.: Especialista em Seguran√ßa, an√°lise de dados).
Arquitetura T√©cnica
Componentes Principais
CrewAI: Framework Python para orquestra√ß√£o de agentes multiagentes, com suporte a tarefas sequenciais e contexto compartilhado.
Ollama: Servidor de infer√™ncia local que executa o Phi-3 Mini, compat√≠vel com a API OpenAI e otimizado para GPUs NVIDIA via CUDA.
Phi-3 Mini: Modelo de linguagem leve (~3.8B par√¢metros, quantizado em 4-bit), ideal para hardware com 4 GB de VRAM.
litellm: Biblioteca usada pelo CrewAI para gerenciar chamadas ao Ollama, configurada para o endpoint local (http://localhost:11434).
Fluxo de Trabalho
Arquiteto de Solu√ß√µes:
Recebe o prompt: "Crie um desenho de solu√ß√£o simples para integrar um tenant B2B com um tenant B2C usando EntraID como IdP, e um ADR com Contexto e Decis√£o."
Gera um desenho textual (ex.: "Tenant B2B -> EntraID -> Tenant B2C") e um ADR estruturado.
Tech Lead:
Usa a sa√≠da do Arquiteto como contexto.
Revisa a arquitetura, sugerindo melhorias (ex.: adicionar API Gateway, corrigir protocolos).
Gerente de Tecnologia:
Usa a arquitetura revisada como contexto.
Produz um relat√≥rio com custo estimado, cronograma e benef√≠cios (ex.: "$5.000/m√™s, 4 meses, seguran√ßa e escalabilidade").
Logs: Todas as etapas s√£o registradas em reasoning_logs.log e exibidas no terminal com verbose=2.
Diagrama de Fluxo
mermaid
graph TD
    A[Arquiteto: Gera Desenho e ADR] --> B[Tech Lead: Revisa Arquitetura]
    B --> C[Gerente: Produz Relat√≥rio]
Requisitos
Hardware
GPU: NVIDIA GeForce GTX 1050 Ti (4 GB VRAM) ou superior.
RAM: 8 GB ou mais.
Armazenamento: ~5 GB para o Ollama e o modelo Phi-3 Mini.
Sistema Operacional: Windows 10/11 (testado), Linux ou macOS (com ajustes).
Software
Python: 3.8 ou superior.
Ollama: Vers√£o mais recente (ollama.ai).
NVIDIA Drivers: Compat√≠veis com CUDA (ex.: vers√£o 546.12 ou superior).
CUDA Toolkit: 11.8 ou 12.x (developer.nvidia.com/cuda-downloads).
Depend√™ncias Python:
bash
pip install crewai litellm openai requests
Instala√ß√£o
Instale o Ollama:
Baixe e instale o Ollama em ollama.ai.
Baixe o modelo Phi-3 Mini:
bash
ollama pull phi3-mini
Configure a GPU:
Instale os drivers NVIDIA para a GTX 1050 Ti (www.nvidia.com/Download).
Instale o CUDA Toolkit 11.8 (developer.nvidia.com/cuda-downloads).
For√ßar o uso da GPU (opcional):
powershell
$env:CUDA_VISIBLE_DEVICES = "0"
Clone o Reposit√≥rio:
bash
git clone https://github.com/seu-usuario/multiagentes.git
cd multiagentes
Crie um Ambiente Virtual:
bash
python -m venv .venv
.\venv\Scripts\activate
Instale Depend√™ncias:
bash
pip install crewai litellm openai requests
Uso
Inicie o Ollama:
Abra um terminal e execute:
bash
ollama serve
Execute o Script:
bash
python agentes_arquitetura_offline.py
Monitore a GPU:
Verifique o uso da GPU durante a execu√ß√£o:
bash
nvidia-smi
Esperado:
Memory-Usage: ~2-3 GB para Phi-3 Mini.
GPU-Util: 50-90%.
Analise os Logs:
Logs detalhados s√£o salvos em reasoning_logs.log e exibidos no terminal.
Exemplo de log:
2025-05-07 10:00:00,123 - INFO - GPU detectada: GeForce GTX 1050 Ti...
2025-05-07 10:00:01,200 - DEBUG - [Arquiteto] Resposta: Desenho: Tenant B2B -> EntraID...
Configura√ß√£o T√©cnica
Modelo Phi-3 Mini
Par√¢metros: ~3.8 bilh√µes.
Quantiza√ß√£o: 4-bit (Q4_0), usando ~2-3 GB de VRAM.
Desempenho: Otimizado para a GTX 1050 Ti (768 n√∫cleos CUDA, CUDA 6.1).
Limita√ß√µes: Sa√≠das simplificadas para tarefas complexas devido ao tamanho do modelo.
Acelera√ß√£o por GPU
CUDA: Configurado para for√ßar o uso da GTX 1050 Ti via CUDA_VISIBLE_DEVICES=0.
Monitoramento: Uso de nvidia-smi para verificar VRAM e utiliza√ß√£o da GPU.
Otimiza√ß√£o: Modelos quantizados e prompts curtos (max_tokens=300) para evitar sobrecarga dos 4 GB de VRAM.
Logging
Biblioteca: logging do Python, com sa√≠das em reasoning_logs.log e terminal.
N√≠vel: DEBUG, capturando prompts, respostas e erros.
Verbose: verbose=2 no CrewAI para rastrear o racioc√≠nio dos agentes.
Exemplo de Sa√≠da
Arquiteto de Solu√ß√µes
Desenho de Solu√ß√£o: O tenant B2B autentica via EntraID usando OAuth 2.0, enquanto o tenant B2C usa OpenID Connect. Uma API Gateway gerencia o tr√°fego.
ADR:
- Contexto: Necessidade de integra√ß√£o segura entre tenants.
- Decis√£o: Usar EntraID como IdP √∫nico.
- Consequ√™ncias: Simplifica gerenciamento, mas exige configura√ß√£o inicial.
Tech Lead
Arquitetura Revisada: Adicionado Azure API Management como Gateway para maior controle de tr√°fego. Corrigido protocolo para OAuth 2.0 em ambos os tenants.
Gerente de Tecnologia
Relat√≥rio:
- Custo Estimado: $5.000/m√™s para infraestrutura Azure.
- Cronograma: 4 meses para implementa√ß√£o.
- Benef√≠cios: Seguran√ßa refor√ßada, escalabilidade e integra√ß√£o simplificada.
Extensibilidade
O sistema √© modular e pode ser expandido para:
Novos Agentes: Ex.: Especialista em Seguran√ßa para revisar vulnerabilidades.
python
especialista_seguranca = Agent(
    role="Especialista em Seguran√ßa",
    goal="Garantir seguran√ßa da arquitetura",
    llm="ollama/phi3-mini"
)
Outros Modelos: Suporte a Mistral-7B (4-bit, ~3-4 GB VRAM) com ajustes:
bash
ollama pull mistral --quantize q4_0
Novos Casos: Planejamento de projetos, an√°lise de dados, ou automa√ß√£o de suporte.
Limita√ß√µes
Hardware: A GTX 1050 Ti (4 GB VRAM) restringe modelos maiores (ex.: LLaMA-3-8B) e m√∫ltiplas itera√ß√µes simult√¢neas.
Phi-3 Mini: Sa√≠das podem ser simplificadas para tarefas t√©cnicas complexas.
Configura√ß√£o: Requer instala√ß√£o manual de drivers NVIDIA, CUDA, e Ollama, o que pode ser desafiador para iniciantes.
Desempenho: Infer√™ncia mais lenta (~segundos por tarefa) devido aos 768 n√∫cleos CUDA da GTX 1050 Ti.
Contribui√ß√µes
Contribui√ß√µes s√£o bem-vindas! Para contribuir:
Fork o reposit√≥rio.
Crie uma branch (git checkout -b feature/nova-funcionalidade).
Commit suas altera√ß√µes (git commit -m "Adiciona nova funcionalidade").
Envie um pull request.
Sugest√µes de melhorias:
Adicionar novos agentes ou tarefas.
Otimizar prompts para o Phi-3 Mini.
Integrar ferramentas de visualiza√ß√£o (ex.: Mermaid, CSV).
Suportar outros modelos ou GPUs.
Licen√ßa
MIT License (LICENSE) <!-- Crie um arquivo LICENSE, se desejar -->
Como Citar
Se usar este projeto em seu trabalho ou pesquisa, cite:
Sistema Multiagentes para Arquitetura de Solu√ß√µes
Autor: [Seu Nome]
Reposit√≥rio: https://github.com/seu-usuario/multiagentes
Contato
Autor: [Seu Nome]
GitHub: seu-usuario
X: [
@seu
-usuario] <!-- Adicione seu handle, se aplic√°vel -->
Notas de Seguran√ßa
Logs: N√£o inclua reasoning_logs.log no reposit√≥rio, pois pode conter dados sens√≠veis.
Offline: O projeto √© 100% offline, garantindo privacidade, mas valide os prompts para evitar exposi√ß√£o de informa√ß√µes corporativas.
.gitignore: Inclui .venv/, *.log, e __pycache__/ para proteger arquivos sens√≠veis.
Passos para Publica√ß√£o no GitHub
Crie o Reposit√≥rio:
Acesse github.com e crie um reposit√≥rio chamado multiagentes.
Escolha P√∫blico (ou Privado, se preferir).
Adicione um README.md inicial (substitua pelo conte√∫do acima).
Organize os Arquivos:
Crie uma pasta local:
powershell
mkdir multiagentes
cd multiagentes
Adicione:
agentes_arquitetura_offline.py (o script principal).
README.md (com o conte√∫do acima).
.gitignore:
.venv/
__pycache__/
*.log
*.pyc
Opcional: LICENSE (ex.: MIT License).
Inicialize o Git:
powershell
git init
git add .
git commit -m "Initial commit: Sistema multiagentes com CrewAI e Ollama"
Conecte ao GitHub:
powershell
git remote add origin https://github.com/seu-usuario/multiagentes.git
git push -u origin main
Valide no GitHub:
Acesse https://github.com/seu-usuario/multiagentes.
Confirme que o README.md est√° renderizado corretamente.
Teste clonando o reposit√≥rio em outro diret√≥rio:
powershell
git clone https://github.com/seu-usuario/multiagentes.git
Divulgue:
Adicione tags no GitHub: AI, multiagent, CrewAI, Ollama, Phi-3, GPU.
Compartilhe no X com #AI, #Multiagent, #Ollama, #CrewAI.
Considere um post com capturas de tela (ex.: nvidia-smi, logs).
Pr√≥ximos Passos
Publique o Reposit√≥rio:
Siga os passos acima e envie o projeto ao GitHub.
Me avise o link do reposit√≥rio para revisar o README.md.
Adicione Melhorias:
Inclua capturas de tela ou GIFs no README.md (use ShareX).
Adicione um agente extra (ex.: Especialista em Seguran√ßa) se quiser expandir.
Crie um arquivo CSV com sa√≠das, como sugerido anteriormente:
python
import csv
outputs = [{"Agente": task.agent.role, "Sa√≠da": task.execute_sync()} for task in equipe.tasks]
with open('agent_outputs.csv', 'w', newline='', encoding='utf-8') as f:
    writer = csv.DictWriter(f, fieldnames=["Agente", "Sa√≠da"])
    writer.writeheader()
    writer.writerows(outputs)
Teste o Mistral-7B:
Se quiser sa√≠das mais robustas, baixe o Mistral-7B (4-bit, ~3-4 GB VRAM):
bash
ollama pull mistral --quantize q4_0
Atualize o c√≥digo:
python
llm="ollama/mistral"
os.environ["OLLAMA_MODEL"] = "mistral"
Monitore a VRAM com nvidia-smi para evitar travamentos.
Feedback:
Compartilhe o link do reposit√≥rio ou trechos das sa√≠das para ajustes.
Se precisar de ajuda com GitHub Actions, imagens, ou outros recursos, posso fornecer instru√ß√µes.
O README.md est√° pronto para destacar a pot√™ncia e a acessibilidade do seu projeto! Se quiser ajustes (ex.: tom mais informal, se√ß√µes extras, ou suporte para outro modelo), √© s√≥ avisar! üöÄ
